#encoding=gb18030
#-*- coding:gb18030-*-
import os
import sys
import logging
import re
import json
import codecs
import urllib
import Queue
import threading
import requests
from lxml import etree
import time
import pymongo
from pymongo import MongoClient
import random
import traceback
from iplist import *
from func import *
import cx_Oracle
from anti_ban_selenium import AntiBan
from selenium.common.exceptions import NoSuchElementException
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.keys import Keys


DB_STR='match/jkn65#ud@192.168.100.111:1521/move' #product
DIR_PATH = sys.path[0] + '/' if sys.path[0] else ''
LOG_FILE = DIR_PATH + 'logs/spider.log'
SPIDER_THREAD_DOWN = False
WEBSITEQUEUE_THREAD_DOWN = False
INTERVALSEC = random.randint(3, 5)
INTERVALSEC_LIST = random.uniform(1, 3)
WAIT_TIME = 20
THREAD_NUM = 1


class WebSite():
    url = ''
    memberId = ''
    appid = -1
    category_count = 0
    page_count = 0
    type = TYPES['INVALID']
    code = -1
    errmsg = ''
    net_time = 0.0
    parse_time = 0.0
    user_defined = True
    has_page = True
    try_count = 2

    def __init__(self, memberid, url, appid, type):
        self.url = url 
        self.memberid = memberid.strip()
        self.appid = appid
        self.type = type 

class WebSiteQueue(threading.Thread):
    __queue = Queue.Queue(0)
    __start_event = threading.Event()
    __mutex = threading.Lock()
    __mutex_record = threading.Lock()

    def __init__(self):
        threading.Thread.__init__(self)
        self.record = {}

    def initialize(self):
        try:
            self.mongo_db = MongoClient('192.168.60.64', 10010).m1688
            self.__mutex.acquire()
            self.oracle_db = cx_Oracle.connect(DB_STR)
            self.__mutex.release()
            return True
        except Exception, e:
            logging.error('init MemberIdQueue failed! err=%s', str(e))
            return False

    def getSize(self):
        return self.__queue.qsize()

    def getWebSite(self):
        return self.__queue.get(True)

    def isEmpty(self):
        return self.__queue.empty()

    def waitStart(self):
        self.__start_event.wait()

    def newWebSite(self, memberid, url, appid, type):
        if memberid and url:
            self.__queue.put(WebSite(memberid, url, appid, type))
            return True
        return False

    def waitEnd(self):
        while self.isAlive():
            time.sleep(1)

    def insert(self, website, records):
        if records and website.memberid:
            try:
                self.mongo_db.content_tbl.remove({'memberId':website.memberid})
                for record in records:
                    try:
                        self.mongo_db.content_tbl.insert(record)
                    except pymongo.errors.DuplicateKeyError, e:
                        logging.warning('insert record duplicatekey error! memberid=%s\n %s', website.memberid, str(record))
                        continue
            except Exception, e:
                logging.error('insert records error url=%s memberid=%s! err=%s', website.url, website.memberid, str(traceback.format_exc()))
                return False
            return True
        else:
            logging.error('insert records error len(records)=%d memberid=%s', len(records), website.memberid)
            return True

    def setStatus(self, website, status):
        try:
            website.status = status
            self.mongo_db.url_tbl.find_one_and_update({'url':website.url}, {'$set':{'status':status, 'count':website.page_count}})
            return True
        except Exception, e:
            logging.error('set status error! err=%s', str(e))
            return False

    def setListPageStatus(self, appid, url, status):
        self.__mutex.acquire()
        while True:
            try:
                if appid > 0:
                    cur = self.oracle_db.cursor()
                    sql_str = "update cor_website_move set SEARCH_LISTPAGE_STATUS=:1 where id=:2 or website=:3"
                    cur.execute(sql_str, (status, appid, url))
                    self.oracle_db.commit()
                    cur.close()
                    self.__mutex.release()
                    break
            except Exception, e:
                logging.error('set list page status error reconnect! sql=%s err=%s', sql_str, str(e))
                self.oracle_db.close()
                self.oracle_db = cx_Oracle.connect(DB_STR)
                continue

    def havingFetchOracle(self, url):
        self.__mutex.acquire()
        while True:
            try:
                sql_str = "select count(1) from cor_website_move where STATES in (0, 7, 9) and SEARCH_LISTPAGE_STATUS = 1 and website=:1"
                cur = self.oracle_db.cursor()
                result = cur.execute(sql_str, (url,))
                items = result.fetchall()
                cur.close()
                self.__mutex.release()
                return bool(items[0][0])
            except Exception, e:
                if str(e).find('ORA-03114') >= 0:
                    self.oracle_db.close()
                    self.oracle_db = cx_Oracle.connect(DB_STR)
                    continue
                logging.error('havingFetchOracle oracle connect error reconnect! sql=%s err=%s', sql_str, str(e))
                return True



    def setListPageStatusFailed(self, appid, url):
        self.setListPageStatus(appid, url, ORACLE_SEARCH_LISTPAGE_STATUS['FAILED'])
        self.deleteRecord(url)

    def setListPageStatusOK(self, appid, url):
        self.setListPageStatus(appid, url, ORACLE_SEARCH_LISTPAGE_STATUS['OK'])
        self.deleteRecord(url)

    def setListPageStatusStandby(self, appid):
        self.setListPageStatus(appid, url, ORACLE_SEARCH_LISTPAGE_STATUS['STANDBY'])

    def setRecord(self, url):
        self.__mutex_record.acquire()
        self.record[url] = 1
        self.__mutex_record.release()

    def hasRecord(self, url):
        self.__mutex_record.acquire()
        ret = url in self.record
        self.__mutex_record.release()
        return ret

    def deleteRecord(self, url):
        self.__mutex_record.acquire()
        self.record.pop(url, 1)
        self.__mutex_record.release()
 
    def run(self):
        try:
            # first set event 
            self.__start_event.set()

            logging.info('read website start!')
            while not WEBSITEQUEUE_THREAD_DOWN:
                logging.info('get url from mongo before')
                items = self.mongo_db.url_tbl.find({'type':{'$in':[TYPES['TAOBAO']]}, 'status':STATUS['STANDBY']}, {'appid':1, 'url':1, 'type':1})
                items = [item for item in items]
                logging.info('get url from oracle after count=%d', len(items))
                if not items:
                    time.sleep(60)
                    continue

                memberIds = []
                taobao_count = tmall_count = other_count = running_count = 0
                for i, item in enumerate(items):
                    while self.getSize() >= 6:
                        time.sleep(60)

                    url = item['url']
                    appid = item['appid']
                    url_type = item['type']
                    if url:
                        logging.info('before url=%s', url)
                        if self.hasRecord(url) or self.havingFetchOracle(url):
                            logging.info('having fetch website appid=%d url=%s type=%d', appid, url, url_type)
                            running_count += 1
                            continue

                        if url_type == TYPES['TAOBAO']:
                            taobao_count += 1
                        elif url_type == TYPES['TMALL']:
                            tmall_count += 1
                        else:
                            other_count += 1
                            continue

                        try:
                            r = requests.get(url, headers=HEADERS_HOMEPAGE)

                            if r.status_code != 200 or len(r.text) <= 0:
                                self.setListPageStatusFailed(appid, url)
                                self.mongo_db.url_tbl.update({'url':url}, {'$set':{'status':STATUS['OK'], 'count':0}})
                                logging.warning('website invalid! url=%s status=%d', url, r.status_code)
                                continue

                            memberId = re.findall(r'(?<=shopId=)\d+(?=;)', r.text)

                            if memberId:
                                self.newWebSite(memberId[0], url, appid, url_type)
                                self.setRecord(url)
                                self.mongo_db.url_tbl.update({'url':url}, {'$set':{'type':url_type, 'memberId':memberId[0]}})
                                logging.info('insert new website url=%s memberid=%s', url, memberId[0])
                            else:
                                self.setListPageStatusFailed(appid, url)
                                self.mongo_db.url_tbl.update({'url':url}, {'$set':{'status':STATUS['OK'], 'count':0}})
                                logging.warning('website invalid not find memberId! url=%s', url)
                        except Exception, e:
                            err_str = str(traceback.format_exc())
                            logging.error('get homepage failed! url=%s err=%s', url, err_str)
                            if err_str.find('Name or service not known') >= 0 or err_str.find('InvalidURL') >= 0 or err_str.find('SSLError') >= 0:
                                self.setListPageStatusFailed(appid, url)
                                self.mongo_db.url_tbl.update({'url':url}, {'$set':{'status':STATUS['OK'], 'count':0}})
                                logging.warning('website invalid! url=%s', url)

                logging.info('read oracle complete! taobao_count=%d tmall_count=%d other_count=%d running_count=%d', \
                    taobao_count, tmall_count, other_count, running_count)
                time.sleep(60)
        except Exception, e:
            self.__start_event.set()
            logging.error('run websitequeue happen error! err=%s', str(traceback.format_exc()))


class Spider(threading.Thread):

    def __init__(self, websitequeue):
        threading.Thread.__init__(self)
        self.websitequeue = websitequeue
        self.wid_lock = threading.Lock()
        self.wid = '13552995480'
        self.ab = AntiBan('chrome', False, True)
        self.broswer = None

    def getCategory(self, memberid):
        if not memberid:
            return []

        category_list = []
        url = 'https://mallbrand.m.tmall.com/service/shop/get_shop_cat.ajsonp?shop_id=%s&parentCatId=0' %(memberid)
        try:
            r = requests.get(url, headers=HEADERS_CATEGORY)
            results = json.loads(r.text[r.text.find('['):r.text.rfind(']')+1]) if r.text else ''
            title_map = {}
            for item in results:
                category_list.append([item['categoriesID'], '', item['categoriesName'], ''])
                title_map[item['categoriesID']] = item['categoriesName']
                if 'subShopCategoryList' in item:
                    for subItem in item['subShopCategoryList']:
                        category_list.append([subItem['categoriesID'], subItem['parentID'],\
                                subItem['categoriesName'], title_map[subItem['parentID']] if subItem['parentID'] in title_map else ''])
        except Exception, e:
            logging.error('get category failed! url=%s err=%s', url, str(e))

        category_list.append(['', '', '', ''])
        return category_list

    def getPageListTaobao(self, website, base_url, catid, catpid):
        pages = []
        pageid = 1
        shopid = website.memberId
        while True:
            try:
                url = base_url %(catpid, catid, catid, pageid)
                r = self.broswer.request('GET', url, allow_redirects=False)
                if r.status_code == 302:
                    global WAIT_TIME
                    logging.warning('get page list 302! shopid=%s pageid=%d catid=%s catpid=%s src_url=%s jump_url=%s wait_time=%d',
                            shopid, pageid, catid, catpid, url, r.headers['location'], WAIT_TIME)
                    time.sleep(WAIT_TIME * 60)
                    continue

                if len(r.text) < 50:
                    self.broswer = self.ab.get_broswer()
                    continue

                if r.text.find(u'ºÜ±§Ç¸£¬ËÑË÷µ½¡°<em>0</em>¡±¸ö±¦±´') > 0:
                    break

                results = re.findall(r'(?<=itemIds=)[\d,]+(?=&)', r.text)
                if not results:
                    logging.warning('get page list not find url! shopid=%s pageid=%d catid=%s catpid=%s url=%s', shopid, pageid, catid, catpid, url)
                    break

                pages.extend(map(lambda x:'https://item.taobao.com/item.htm?id=' + x, results[0].split(',')))

                pageid += 1

                time.sleep(INTERVALSEC_LIST)
            except requests.exceptions.Timeout:
                logging.warning('get page list time out! retry it! url=%s shopid=%s count=%d err=%s', url, shopid, len(pages), str(traceback.format_exc()))
                continue
            except Exception, e:
                if str(e).find('Connection timed out') >= 0 or str(e).find('Name or service not known') >=0:
                    logging.warning('get page list time out! retry it! url=%s shopid=%s count=%d err=%s', url, shopid, len(pages), str(traceback.format_exc()))
                    continue
                else:
                    logging.warning('get page list failed! url=%s shopid=%s count=%d err=%s', url, shopid, len(pages), str(traceback.format_exc()))
                    break

        return set(pages)

    def get_all_urls(self):
        all_urls = []
        while True:
            try:
                time.sleep(1)
                if self.broswer.page_source.find(u'Î´ÕÒµ½·ûºÏµÄ±¦±´') > 0:
                    break
                eles_page = self.broswer.find_elements_by_xpath("//ul[@id='js-goods-list-items']/li/a")
            except NoSuchElementException:
                break

            urls = [ele.get_attribute('href').replace('h5.m.taobao.com/awp/core/detail.htm', 'item.taobao.com/item.htm')
                    for ele in eles_page]
            all_urls.extend(urls)

            ## click next page
            while True:
                try:
                    for i in range(15):
                        ActionChains(self.broswer).key_down(Keys.PAGE_DOWN).perform()
                        time.sleep(0.1)
                    self.broswer.find_element_by_xpath("//a[@class='c-btn c-btn-awr c-p-next']").click()
                    break
                except NoSuchElementException:
                    return all_urls
                except Exception as e:
                    if e.message.find('not clickable') > 0:
                        continue

        return all_urls

    def combine_records(self, records, website, catid, catpid, catname, catpname, urls):
        website.page_count = website.page_count + len(urls)
        records.append({
            'memberId': website.memberid,
            'catid': catid,
            'catpid': catpid,
            'catname': catname,
            'catpname': catpname,
            'count':len(urls),
            'status':1,
            'updateTime':time.time(),
            'urls':reduce(lambda x,y: x+'|'+y, urls) if urls else ''
        })

    def run(self):
        while not SPIDER_THREAD_DOWN:
            website = self.websitequeue.getWebSite()

            # 1. get category
            records = []
            category_list = self.getCategory(website.memberid)
            website.category_count = len(category_list)
            if not category_list:
                category_list = [['', '', '', '']]
            logging.info('get category complete! url=%s memberid=%s category_count=%d',
                         website.url, website.memberid, website.category_count)

            while True:
                try:
                    # 2. get pagelist from category
                    logging.info('open homepage with selenium url=%s memberid=%s', website.url, website.memberid)
                    self.broswer = self.ab.get_broswer()
                    self.broswer.get(website.url)
                    ## all category
                    self.broswer.find_element_by_xpath("//span[@class='js-nav-csch fun csch']").click()
                    time.sleep(1)
                    eles = self.broswer.find_elements_by_xpath("//ul[@class='menu']/li")
                    for index, ele in enumerate(eles):
                        if index == 0 and len(eles) > 1:
                            continue
                        ## master category
                        if index > 1:
                            self.broswer.find_element_by_xpath("//span[@class='js-nav-csch fun csch']").click()
                            time.sleep(1)
                        ele_master = ele.find_element_by_xpath("./h2/span")
                        catpid = ele_master.get_attribute('data-id')
                        catpname = ele_master.text
                        time.sleep(1)
                        ele_master.click()
                        urls = self.get_all_urls()
                        logging.info('getpagelist one category complete. [master] url=%s memberid=%s catid=%s '
                                     'catname=%s count=%d',
                                     website.url, website.memberid, catpid, catpname, len(urls))
                        self.combine_records(records, website, catpid, '', catpname, '', urls)

                        ## slaves category
                        try:
                            ele_slaves = ele.find_elements_by_xpath("./ul/li")
                        except NoSuchElementException:
                            continue

                        for ele in ele_slaves:
                            self.broswer.find_element_by_xpath("//span[@class='js-nav-csch fun csch']").click()
                            time.sleep(1)
                            catid = ele.get_attribute('data-id')
                            catname = ele.text
                            time.sleep(1)
                            ele.click()
                            time.sleep(1)
                            urls = self.get_all_urls()
                            logging.info('getpagelist one category complete. [slaves] url=%s memberid=%s catid=%s catname=%s '
                                         'catpid=%s catpname=%s count=%d',
                                         website.url, website.memberid, catpid, catname, catpid, catpname, len(urls))
                            self.combine_records(records, website, catid, catpid, catname, catpname, urls)

                    # 3. write to mongo and change status from oracle
                    if websitequeue.insert(website, records):
                        if website.page_count > 0:
                            websitequeue.setListPageStatusOK(website.appid, website.url)
                        else:
                            websitequeue.setListPageStatusFailed(website.appid, website.url)
                        websitequeue.setStatus(website, STATUS['OK'])
                        logging.info('insert records succeed url=%s memberid=%s category_count=%d page_count=%d', \
                            website.url, website.memberid, website.category_count, website.page_count)
                    break
                except Exception, e:
                    logging.error('something error! %s', str(e))
                    continue

if __name__ == '__main__':
    #daemonize('/dev/null', LOG_FILE, LOG_FILE)
    # set global coding to gb18030
    reload(sys)
    sys.setdefaultencoding('gb18030')

    logInit(logging.INFO, LOG_FILE, 0, True)
    logging.info('start')

    websitequeue = WebSiteQueue()
    websitequeue.daemon = True
    if not websitequeue.initialize():
        sys.exit(1)
    websitequeue.start()
    websitequeue.waitStart()

    # start spider thread
    spiders = []
    for i in range(THREAD_NUM):
        s = Spider(websitequeue)
        s.daemon = True
        spiders.append(s)
        s.start()

    websitequeue.waitEnd()

    logging.info('end')
